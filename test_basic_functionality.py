#!/usr/bin/env python3
"""
åŸºç¡€åŠŸèƒ½æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ç¯å¢ƒã€æ™ºèƒ½ä½“ç­‰åŸºæœ¬ç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import sys
import os
import torch
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from envs.gridworld import GridWorld
from agents.rtd_agent import RTDAgent
from agents.baseline_agents import QMIXAgent

def test_environment():
    """æµ‹è¯•ç¯å¢ƒåŸºæœ¬åŠŸèƒ½"""
    print("=" * 50)
    print("æµ‹è¯•ç¯å¢ƒåŸºæœ¬åŠŸèƒ½")
    print("=" * 50)
    
    try:
        # åˆ›å»ºç¯å¢ƒ
        env = GridWorld(size=3, num_agents=2, max_steps=10)
        print("âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•é‡ç½®
        obs = env.reset()
        print(f"âœ“ ç¯å¢ƒé‡ç½®æˆåŠŸï¼Œè§‚å¯Ÿå€¼: {obs}")
        print(f"âœ“ è§‚å¯Ÿå€¼ç±»å‹: {type(obs)}, é•¿åº¦: {len(obs) if obs else 'None'}")
        
        # æµ‹è¯•æ­¥è¿›
        actions = [0, 1]  # ä¸¤ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œ
        result = env.step(actions)
        print(f"âœ“ ç¯å¢ƒæ­¥è¿›æˆåŠŸï¼Œè¿”å›å€¼: {result}")
        print(f"âœ“ è¿”å›å€¼ç±»å‹: {type(result)}, é•¿åº¦: {len(result)}")
        
        # éªŒè¯è¿”å›å€¼
        if len(result) == 4:
            new_obs, rewards, done, info = result
            print(f"âœ“ æ–°è§‚å¯Ÿå€¼: {new_obs}")
            print(f"âœ“ å¥–åŠ±: {rewards}")
            print(f"âœ“ å®ŒæˆçŠ¶æ€: {done}")
            print(f"âœ“ ä¿¡æ¯: {info}")
        else:
            print(f"âœ— è¿”å›å€¼é•¿åº¦ä¸æ­£ç¡®: {len(result)}")
            return False
            
        return True
        
    except Exception as e:
        print(f"âœ— ç¯å¢ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_rtd_agent():
    """æµ‹è¯•RTDæ™ºèƒ½ä½“åŸºæœ¬åŠŸèƒ½"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•RTDæ™ºèƒ½ä½“åŸºæœ¬åŠŸèƒ½")
    print("=" * 50)
    
    try:
        # åˆ›å»ºæ™ºèƒ½ä½“
        config = {
            "env": {"num_agents": 2},
            "rtd": {
                "regret_threshold": 1.0,
                "delay_queue_size": 5,
                "ensemble_size": 2,
                "policy_change_window": 5,
                "kl_threshold": 0.1,
                "lambda_min": 0.1,
                "lambda_max": 2.0,
                "lambda_kappa": 1.0,
                "lambda_tau": 0.5,
                "individual_regret_weight": 0.6,
                "team_regret_weight": 0.4,
                "regret_learning_rate": 0.01,
                "cooperation_window": 10,
                "reward_covariance_threshold": 0.1,
                "marginal_gain_threshold": 0.05,
                "cooperation_smoothing": 0.9,
                "alpha_base": 0.2,
                "beta_base": 0.8,
                "alpha_delta": 0.1,
                "beta_delta": 0.1,
                "communication_budget": 50,
                "message_cost": 1,
                "voi_threshold": 0.1,
                "delay_domain_steps": 2,
                "guardian_action_prob": 0.3
            }
        }
        
        agent = RTDAgent(obs_dim=2, action_dim=5, config=config, agent_id=0)
        print("âœ“ RTDæ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
        obs = [1, 2]  # ç¤ºä¾‹è§‚å¯Ÿå€¼
        other_agents_obs = [[0, 1]]
        other_agents_actions = [2]
        
        result = agent.act(obs, other_agents_obs, other_agents_actions)
        print(f"âœ“ åŠ¨ä½œé€‰æ‹©æˆåŠŸï¼Œè¿”å›å€¼: {result}")
        print(f"âœ“ è¿”å›å€¼ç±»å‹: {type(result)}, é•¿åº¦: {len(result)}")
        
        # éªŒè¯è¿”å›å€¼
        if len(result) == 3:
            action, decision, regret = result
            print(f"âœ“ åŠ¨ä½œ: {action}")
            print(f"âœ“ å†³ç­–: {decision}")
            print(f"âœ“ åæ‚”å€¼: {regret}")
        else:
            print(f"âœ— è¿”å›å€¼é•¿åº¦ä¸æ­£ç¡®: {len(result)}")
            return False
            
        return True
        
    except Exception as e:
        print(f"âœ— RTDæ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_baseline_agent():
    """æµ‹è¯•åŸºçº¿æ™ºèƒ½ä½“åŸºæœ¬åŠŸèƒ½"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•åŸºçº¿æ™ºèƒ½ä½“åŸºæœ¬åŠŸèƒ½")
    print("=" * 50)
    
    try:
        # åˆ›å»ºæ™ºèƒ½ä½“
        config = {"env": {"num_agents": 2}}
        agent = QMIXAgent(obs_dim=2, action_dim=5, config=config, agent_id=0)
        print("âœ“ QMIXæ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
        obs = [1, 2]  # ç¤ºä¾‹è§‚å¯Ÿå€¼
        other_agents_obs = [[0, 1]]
        other_agents_actions = [2]
        
        result = agent.act(obs, other_agents_obs, other_agents_actions)
        print(f"âœ“ åŠ¨ä½œé€‰æ‹©æˆåŠŸï¼Œè¿”å›å€¼: {result}")
        print(f"âœ“ è¿”å›å€¼ç±»å‹: {type(result)}, é•¿åº¦: {len(result)}")
        
        # éªŒè¯è¿”å›å€¼
        if len(result) == 3:
            action, decision, regret = result
            print(f"âœ“ åŠ¨ä½œ: {action}")
            print(f"âœ“ å†³ç­–: {decision}")
            print(f"âœ“ åæ‚”å€¼: {regret}")
        else:
            print(f"âœ— è¿”å›å€¼é•¿åº¦ä¸æ­£ç¡®: {len(result)}")
            return False
            
        return True
        
    except Exception as e:
        print(f"âœ— åŸºçº¿æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_simple_interaction():
    """æµ‹è¯•ç®€å•çš„æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç®€å•çš„æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’")
    print("=" * 50)
    
    try:
        # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
        env = GridWorld(size=3, num_agents=2, max_steps=5)
        config = {
            "env": {"num_agents": 2},
            "rtd": {
                "ensemble_size": 3,
                "delay_queue_size": 10,
                "communication_budget": 100,
                "message_cost": 1,
                "voi_threshold": 0.1,
                "delay_domain_steps": 5,
                "guardian_action_prob": 0.3,
                "alpha_base": 0.1,
                "beta_base": 0.2,
                "alpha_delta": 0.05,
                "beta_delta": 0.05,
                "individual_regret_weight": 0.6,
                "team_regret_weight": 0.4,
                "cooperation_window": 10,
                "policy_change_window": 20,
                "regret_threshold": 1.0,
                "kl_threshold": 0.1,
                "lambda_min": 0.1,
                "lambda_max": 2.0,
                "lambda_kappa": 1.0,
                "lambda_tau": 0.5,
                "regret_learning_rate": 0.01,
                "reward_covariance_threshold": 0.1,
                "marginal_gain_threshold": 0.05,
                "cooperation_smoothing": 0.9
            }
        }
        
        rtd_agent = RTDAgent(obs_dim=2, action_dim=5, config=config, agent_id=0)
        qmix_agent = QMIXAgent(obs_dim=2, action_dim=5, config=config, agent_id=1)
        
        agents = [rtd_agent, qmix_agent]
        print("âœ“ ç¯å¢ƒå’Œæ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
        
        # è¿è¡Œä¸€ä¸ªç®€å•çš„episode
        obs = env.reset()
        print(f"âœ“ ç¯å¢ƒé‡ç½®ï¼Œåˆå§‹è§‚å¯Ÿå€¼: {obs}")
        
        for step in range(3):  # åªè¿è¡Œ3æ­¥
            print(f"\n--- Step {step + 1} ---")
            
            # æ”¶é›†åŠ¨ä½œ
            actions = []
            for i, agent in enumerate(agents):
                other_agents_obs = [obs[j] for j in range(len(agents)) if j != i]
                # è¿‡æ»¤æ‰Noneå€¼ï¼Œåªä¿ç•™æœ‰æ•ˆçš„åŠ¨ä½œ
                other_agents_actions = [actions[j] for j in range(len(agents)) if j < len(actions) and actions[j] is not None]
                
                result = agent.act(obs[i], other_agents_obs, other_agents_actions)
                action, decision, regret = result
                actions.append(action)
                
                print(f"æ™ºèƒ½ä½“ {i} ({type(agent).__name__}): åŠ¨ä½œ={action}, å†³ç­–={decision}, åæ‚”={regret}")
            
            # ç¯å¢ƒæ­¥è¿›
            new_obs, rewards, done, info = env.step(actions)
            print(f"ç¯å¢ƒæ­¥è¿›: å¥–åŠ±={rewards}, å®Œæˆ={done}")
            
            # æ›´æ–°è§‚å¯Ÿå€¼
            obs = new_obs
            
            if done:
                print("Episodeå®Œæˆ")
                break
        
        print("âœ“ ç®€å•äº¤äº’æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âœ— ç®€å•äº¤äº’æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹åŸºç¡€åŠŸèƒ½æµ‹è¯•...")
    
    tests = [
        test_environment,
        test_rtd_agent,
        test_baseline_agent,
        test_simple_interaction
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            if test():
                passed += 1
                print("âœ“ æµ‹è¯•é€šè¿‡")
            else:
                print("âœ— æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âœ— æµ‹è¯•å¼‚å¸¸: {e}")
    
    print(f"\n" + "=" * 50)
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åŸºç¡€åŠŸèƒ½æ­£å¸¸")
        return True
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥ä»£ç ")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
